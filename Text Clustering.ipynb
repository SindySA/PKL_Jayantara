{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "70b0be47-34fd-43d1-8be2-ebb4b47eb7be",
   "metadata": {},
   "source": [
    "Sumber text clustering using NLTK library: https://github.com/lucas-de-sa/national-anthems-clustering/blob/master/Cluster_Anthems.ipynb\n",
    "\n",
    "Sumber text clustering using Spacy library: https://github.com/kirralabs/text-clustering/blob/master/script/core/Clustering.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9eaaf42c-07e2-4bb3-995e-718721b14792",
   "metadata": {},
   "source": [
    "# A. Import Library"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e67c880f-3f58-498d-bc1f-eed2fde152fa",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### a). Library if use NLTK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d378beda-1730-45bb-a019-747113809a8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Structures\n",
    "import numpy  as np\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "import json\n",
    "\n",
    "# Corpus Processing\n",
    "import re\n",
    "import nltk.corpus\n",
    "from unidecode                        import unidecode\n",
    "from nltk.tokenize                    import word_tokenize\n",
    "from nltk                             import SnowballStemmer\n",
    "from sklearn.feature_extraction.text  import TfidfVectorizer\n",
    "from sklearn.preprocessing            import normalize\n",
    "\n",
    "# K-Means\n",
    "from sklearn import cluster\n",
    "\n",
    "# Visualization and Analysis\n",
    "import matplotlib.pyplot  as plt\n",
    "import matplotlib.cm      as cm\n",
    "import seaborn            as sns\n",
    "from sklearn.metrics                  import silhouette_samples, silhouette_score\n",
    "from wordcloud                        import WordCloud\n",
    "\n",
    "# Map Viz\n",
    "import folium\n",
    "#import branca.colormap as cm\n",
    "from branca.element import Figure"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c4717a3-952a-4e00-8315-d06800406e3c",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### b). Library if use Spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "c8c36752-bf67-4740-9895-659c27981013",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, mpld3, nltk\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "4db7c77f-be18-485e-bd9d-95ec13bf25dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from spacy.lang.id import Indonesian\n",
    "import fnmatch\n",
    "def getAllFileinFolder(folderpath):\n",
    "    filelist = []\n",
    "    for dirpath, dirs, files in os.walk(folderpath):\n",
    "        for filename in fnmatch.filter(files, '*.txt'):\n",
    "            filelist.append(dirpath + \"/\" + filename)\n",
    "    return filelist\n",
    "\n",
    "def writedataa(list, thname):\n",
    "    file = open(\"sentence_rep_{}.txt\".format(thname), \"w\");\n",
    "    for x in sorted(set(list)):\n",
    "        # for x in list:\n",
    "        # hasil = x.replace('\"','').replace(\"#\",\"\").replace(\"&nbsp;\",\"\" )\n",
    "        file.write(x + \"\\n\")\n",
    "    file.close()\n",
    "\n",
    "nlp = Indonesian()\n",
    "def tokenize_and_stem(text):\n",
    "    text = u'{}'.format(text)\n",
    "    doc = nlp(text)\n",
    "    stems = [t.lemma_ for t in doc]\n",
    "    stems = [t.lower() for t in stems]\n",
    "    return stems\n",
    "\n",
    "\n",
    "def tokenize_only(text):\n",
    "    text = u'{}'.format(text)\n",
    "    doc = nlp(text)\n",
    "    stems = [t.text for t in doc]\n",
    "    stems = [t.lower() for t in stems]\n",
    "    return stems"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4464e468-4fb2-4585-afb8-ba835b8b9541",
   "metadata": {},
   "source": [
    "# B. Import Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "51c00119-0d6a-4832-9d5e-1c88cfa78356",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sumber</th>\n",
       "      <th>tahun bulan</th>\n",
       "      <th>issue id</th>\n",
       "      <th>tracker</th>\n",
       "      <th>created on</th>\n",
       "      <th>closed on</th>\n",
       "      <th>start date</th>\n",
       "      <th>due date</th>\n",
       "      <th>project</th>\n",
       "      <th>subject</th>\n",
       "      <th>state</th>\n",
       "      <th>poin</th>\n",
       "      <th>id resolver</th>\n",
       "      <th>nama resolver</th>\n",
       "      <th>grup resolver</th>\n",
       "      <th>id creator</th>\n",
       "      <th>nama creator</th>\n",
       "      <th>ts menit</th>\n",
       "      <th>wkt resolved</th>\n",
       "      <th>st lembur</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Lapor PPDB</td>\n",
       "      <td>2024-06</td>\n",
       "      <td>21639</td>\n",
       "      <td>Support</td>\n",
       "      <td>2024-06-29 09:09:00</td>\n",
       "      <td>2024-07-01 05:11:11</td>\n",
       "      <td>2024-06-29</td>\n",
       "      <td>2024-06-29</td>\n",
       "      <td>Kota Denpasar</td>\n",
       "      <td>[Closing Engine] Kota Denpasar 2024 - Jalur Zo...</td>\n",
       "      <td>Closed</td>\n",
       "      <td>NaN</td>\n",
       "      <td>69.0</td>\n",
       "      <td>Febrian Hilmi Firdaus</td>\n",
       "      <td>DSO</td>\n",
       "      <td>69</td>\n",
       "      <td>Febrian Hilmi Firdaus</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2024-06-29 14:30:41</td>\n",
       "      <td>Lembur</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Lapor PPDB</td>\n",
       "      <td>2024-06</td>\n",
       "      <td>21662</td>\n",
       "      <td>Support</td>\n",
       "      <td>2024-06-29 12:51:50</td>\n",
       "      <td>2024-07-01 05:10:57</td>\n",
       "      <td>2024-06-29</td>\n",
       "      <td>2024-06-29</td>\n",
       "      <td>Prov. Bali</td>\n",
       "      <td>[Kendala] Siswa Masih Bisa Langsung Pengajuan ...</td>\n",
       "      <td>Closed</td>\n",
       "      <td>NaN</td>\n",
       "      <td>69.0</td>\n",
       "      <td>Febrian Hilmi Firdaus</td>\n",
       "      <td>DSO</td>\n",
       "      <td>69</td>\n",
       "      <td>Febrian Hilmi Firdaus</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2024-07-01 05:10:57</td>\n",
       "      <td>Normal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Lapor PPDB</td>\n",
       "      <td>2024-06</td>\n",
       "      <td>21088</td>\n",
       "      <td>Support</td>\n",
       "      <td>2024-06-22 05:30:34</td>\n",
       "      <td>2024-06-22 10:18:51</td>\n",
       "      <td>2024-06-22</td>\n",
       "      <td>2024-06-29</td>\n",
       "      <td>Prov. Nusa Tenggara Timur</td>\n",
       "      <td>[Closing Engine] Prov NTT 2024 - SMA All Jalur...</td>\n",
       "      <td>Closed</td>\n",
       "      <td>NaN</td>\n",
       "      <td>69.0</td>\n",
       "      <td>Febrian Hilmi Firdaus</td>\n",
       "      <td>DSO</td>\n",
       "      <td>69</td>\n",
       "      <td>Febrian Hilmi Firdaus</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2024-06-22 08:54:49</td>\n",
       "      <td>Normal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Lapor PPDB</td>\n",
       "      <td>2024-06</td>\n",
       "      <td>21689</td>\n",
       "      <td>Support</td>\n",
       "      <td>2024-07-01 02:28:58</td>\n",
       "      <td>2024-07-01 02:46:11</td>\n",
       "      <td>2024-06-30</td>\n",
       "      <td>2024-07-01</td>\n",
       "      <td>Kanwil Prov. DKI Jakarta</td>\n",
       "      <td>[DEV] Pengecekan dan Penyesuaian hasil seleksi...</td>\n",
       "      <td>Closed</td>\n",
       "      <td>5.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>Azhar Mashuri</td>\n",
       "      <td>DIP</td>\n",
       "      <td>4</td>\n",
       "      <td>Azhar Mashuri</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2024-07-01 02:46:06</td>\n",
       "      <td>Lembur</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Lapor PPDB</td>\n",
       "      <td>2024-06</td>\n",
       "      <td>21479</td>\n",
       "      <td>Support</td>\n",
       "      <td>2024-06-26 14:13:47</td>\n",
       "      <td>2024-07-01 01:27:33</td>\n",
       "      <td>2024-06-26</td>\n",
       "      <td>2024-06-26</td>\n",
       "      <td>Kota Sukabumi</td>\n",
       "      <td>Pengecekan File Ajuan Pendaftaran di Operator ...</td>\n",
       "      <td>Closed</td>\n",
       "      <td>NaN</td>\n",
       "      <td>74.0</td>\n",
       "      <td>Hayan .</td>\n",
       "      <td>DSO</td>\n",
       "      <td>74</td>\n",
       "      <td>Hayan .</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2024-06-26 14:24:31</td>\n",
       "      <td>Normal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Lapor PPDB</td>\n",
       "      <td>2024-06</td>\n",
       "      <td>21680</td>\n",
       "      <td>Support</td>\n",
       "      <td>2024-06-30 19:56:25</td>\n",
       "      <td>2024-06-30 22:43:27</td>\n",
       "      <td>2024-06-30</td>\n",
       "      <td>2024-06-30</td>\n",
       "      <td>Kota Serang</td>\n",
       "      <td>[DEV] Backend - Tolak Pilihan Provinsi Dan Kot...</td>\n",
       "      <td>Closed</td>\n",
       "      <td>1.0</td>\n",
       "      <td>62.0</td>\n",
       "      <td>Villa Nanda</td>\n",
       "      <td>DIP</td>\n",
       "      <td>4</td>\n",
       "      <td>Azhar Mashuri</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2024-06-30 21:41:11</td>\n",
       "      <td>Lembur</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       sumber tahun bulan  issue id  tracker           created on  \\\n",
       "0  Lapor PPDB     2024-06     21639  Support  2024-06-29 09:09:00   \n",
       "1  Lapor PPDB     2024-06     21662  Support  2024-06-29 12:51:50   \n",
       "2  Lapor PPDB     2024-06     21088  Support  2024-06-22 05:30:34   \n",
       "3  Lapor PPDB     2024-06     21689  Support  2024-07-01 02:28:58   \n",
       "4  Lapor PPDB     2024-06     21479  Support  2024-06-26 14:13:47   \n",
       "5  Lapor PPDB     2024-06     21680  Support  2024-06-30 19:56:25   \n",
       "\n",
       "             closed on  start date    due date                    project  \\\n",
       "0  2024-07-01 05:11:11  2024-06-29  2024-06-29              Kota Denpasar   \n",
       "1  2024-07-01 05:10:57  2024-06-29  2024-06-29                 Prov. Bali   \n",
       "2  2024-06-22 10:18:51  2024-06-22  2024-06-29  Prov. Nusa Tenggara Timur   \n",
       "3  2024-07-01 02:46:11  2024-06-30  2024-07-01   Kanwil Prov. DKI Jakarta   \n",
       "4  2024-07-01 01:27:33  2024-06-26  2024-06-26              Kota Sukabumi   \n",
       "5  2024-06-30 22:43:27  2024-06-30  2024-06-30                Kota Serang   \n",
       "\n",
       "                                             subject   state  poin  \\\n",
       "0  [Closing Engine] Kota Denpasar 2024 - Jalur Zo...  Closed   NaN   \n",
       "1  [Kendala] Siswa Masih Bisa Langsung Pengajuan ...  Closed   NaN   \n",
       "2  [Closing Engine] Prov NTT 2024 - SMA All Jalur...  Closed   NaN   \n",
       "3  [DEV] Pengecekan dan Penyesuaian hasil seleksi...  Closed   5.0   \n",
       "4  Pengecekan File Ajuan Pendaftaran di Operator ...  Closed   NaN   \n",
       "5  [DEV] Backend - Tolak Pilihan Provinsi Dan Kot...  Closed   1.0   \n",
       "\n",
       "   id resolver          nama resolver grup resolver  id creator  \\\n",
       "0         69.0  Febrian Hilmi Firdaus           DSO          69   \n",
       "1         69.0  Febrian Hilmi Firdaus           DSO          69   \n",
       "2         69.0  Febrian Hilmi Firdaus           DSO          69   \n",
       "3          4.0          Azhar Mashuri           DIP           4   \n",
       "4         74.0                Hayan .           DSO          74   \n",
       "5         62.0            Villa Nanda           DIP           4   \n",
       "\n",
       "            nama creator  ts menit         wkt resolved st lembur  \n",
       "0  Febrian Hilmi Firdaus       NaN  2024-06-29 14:30:41    Lembur  \n",
       "1  Febrian Hilmi Firdaus       NaN  2024-07-01 05:10:57    Normal  \n",
       "2  Febrian Hilmi Firdaus       NaN  2024-06-22 08:54:49    Normal  \n",
       "3          Azhar Mashuri       NaN  2024-07-01 02:46:06    Lembur  \n",
       "4                Hayan .       NaN  2024-06-26 14:24:31    Normal  \n",
       "5          Azhar Mashuri       NaN  2024-06-30 21:41:11    Lembur  "
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv('~/Downloads/data-ticket-karyawan.csv', encoding='utf-8')\n",
    "data.columns = map(str.lower, data.columns) #membuat huruf awal nama kolom menjadi tidak kapital\n",
    "\n",
    "sumber = ['Lapor PPDB']\n",
    "data = data.loc[data['sumber'].isin(sumber)]\n",
    "data.head(6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "e337b4de-fd75-40b2-a51e-62de6a28c20b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 2036 entries, 0 to 2035\n",
      "Data columns (total 20 columns):\n",
      " #   Column         Non-Null Count  Dtype  \n",
      "---  ------         --------------  -----  \n",
      " 0   sumber         2036 non-null   object \n",
      " 1   tahun bulan    2036 non-null   object \n",
      " 2   issue id       2036 non-null   int64  \n",
      " 3   tracker        2036 non-null   object \n",
      " 4   created on     2036 non-null   object \n",
      " 5   closed on      2036 non-null   object \n",
      " 6   start date     1630 non-null   object \n",
      " 7   due date       2036 non-null   object \n",
      " 8   project        2036 non-null   object \n",
      " 9   subject        2036 non-null   object \n",
      " 10  state          2036 non-null   object \n",
      " 11  poin           1712 non-null   float64\n",
      " 12  id resolver    2036 non-null   float64\n",
      " 13  nama resolver  2036 non-null   object \n",
      " 14  grup resolver  1952 non-null   object \n",
      " 15  id creator     2036 non-null   int64  \n",
      " 16  nama creator   2036 non-null   object \n",
      " 17  ts menit       371 non-null    float64\n",
      " 18  wkt resolved   2036 non-null   object \n",
      " 19  st lembur      2036 non-null   object \n",
      "dtypes: float64(3), int64(2), object(15)\n",
      "memory usage: 334.0+ KB\n"
     ]
    }
   ],
   "source": [
    "data.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df70d4bc-d3a5-42d4-a4a9-4e3812d0aac3",
   "metadata": {},
   "source": [
    "# Carpus Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba658527-f9e5-4cca-a5b5-103aff9b3418",
   "metadata": {},
   "source": [
    "### Corpus Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "63b9d69e-a1de-4a17-a62d-5a187c89bd95",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Override - Batasan Ajuan Pendaftaran Jalur Zonasi, PTO dan Prestasi'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus = data['subject'].tolist()\n",
    "corpus[18][0:447]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb51f766-a67a-496f-aa29-be983a6ec047",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## 1. Stop Words and Stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3e0fc722-a0ff-417f-94a6-2ee41ddd2769",
   "metadata": {},
   "outputs": [],
   "source": [
    "# removes a list of words (ie. stopwords) from a tokenized list.\n",
    "def removeWords(listOfTokens, listOfWords):\n",
    "    return [token for token in listOfTokens if token not in listOfWords]\n",
    "\n",
    "# applies stemming to a list of tokenized words\n",
    "def applyStemming(listOfTokens, stemmer):\n",
    "    return [stemmer.stem(token) for token in listOfTokens]\n",
    "\n",
    "# removes any words composed of less than 2 or more than 21 letters\n",
    "def twoLetters(listOfTokens):\n",
    "    twoLetterWord = []\n",
    "    for token in listOfTokens:\n",
    "        if len(token) <= 2 or len(token) >= 21:\n",
    "            twoLetterWord.append(token)\n",
    "    return twoLetterWord"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff50dbb1-5cae-4ea5-90dc-339f30169021",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## 2. The main corpus processing function"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c109239d-555d-4c4f-89fc-192f4fda187c",
   "metadata": {},
   "source": [
    "STOPWORDS UNTUK BAHASA INDONESIA: https://github.com/stopwords-iso/stopwords-id/blob/132f51cb383abf95a98beb33c4e1afb6933ff884/raw/indonesian-stopwords-complete.txt\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94e2eda3-b160-43c6-b554-2f8cd8914412",
   "metadata": {},
   "source": [
    "Tutorial install nltk: https://www.google.com/search?sca_esv=67eac9aa2b9499b8&sxsrf=ADLYWIKFMpDR9XEWQVPMLfUljnd_t7hZSw:1721699518919&q=nltk+for+mac&tbm=vid&source=lnms&fbs=AEQNm0DVrIRjdA3gRKfJJ-deMT8ZtYOjoIt1NWOMRkEKym4u5PkAZgxJOmIgPx6WieMhF6q1Hq7W6nME2Vp0eHuijF3ZElaTgD0zbj1gkQrti2r6HpgEQJ__FI2P2zVbzOTQnx-xQGuWfPA7_LjHL8X54xCjPigLtLX638JLYGhCvRlpvvGBo-fNpc7q_rU8dgffCadMYeMgxPqmupqDpgcFpVxKo2EBMA&sa=X&ved=2ahUKEwjcuvDBhryHAxXGVmwGHXduBboQ0pQJegQIDBAB&biw=1393&bih=701&dpr=2#fpstate=ive&vld=cid:1e6e69e9,vid:wuQeKgXUZks,st:0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "3c90055f-2283-4df6-8205-3f7e6ec0f365",
   "metadata": {},
   "outputs": [],
   "source": [
    "def processCorpus(corpus, language):   \n",
    "    #stopwords = nltk.corpus.stopwords.words(language)\n",
    "    param_stemmer = SnowballStemmer(language)\n",
    "    #countries_list = [line.rstrip('\\n') for line in open('lists/countries.txt')] # Load .txt file line by line\n",
    "    #nationalities_list = [line.rstrip('\\n') for line in open('lists/nationalities.txt')] # Load .txt file line by line\n",
    "    #other_words = [line.rstrip('\\n') for line in open('lists/stopwords_scrapmaker.txt')] # Load .txt file line by line\n",
    "    \n",
    "    for document in corpus:\n",
    "        index = corpus.index(document)\n",
    "        #corpus[index] = corpus[index].replace(u'\\ufffd', '8')   # Replaces the ASCII '�' symbol with '8'\n",
    "        corpus[index] = corpus[index].replace(',', '')          # Removes commas\n",
    "        corpus[index] = corpus[index].rstrip('\\n')              # Removes line breaks\n",
    "        corpus[index] = corpus[index].casefold()                # Makes all letters lowercase\n",
    "        \n",
    "        #corpus[index] = re.sub('\\W_',' ', corpus[index])        # removes specials characters and leaves only words\n",
    "        #corpus[index] = re.sub(\"\\S*\\d\\S*\",\" \", corpus[index])   # removes numbers and words concatenated with numbers IE h4ck3r. Removes road names such as BR-381.\n",
    "        #corpus[index] = re.sub(\"\\S*@\\S*\\s?\",\" \", corpus[index]) # removes emails and mentions (words with @)\n",
    "        #corpus[index] = re.sub(r'http\\S+', '', corpus[index])   # removes URLs with http\n",
    "        #corpus[index] = re.sub(r'www\\S+', '', corpus[index])    # removes URLs with www\n",
    "\n",
    "        listOfTokens = word_tokenize(corpus[index])\n",
    "        twoLetterWord = twoLetters(listOfTokens)\n",
    "\n",
    "        #listOfTokens = removeWords(listOfTokens, stopwords)\n",
    "        listOfTokens = removeWords(listOfTokens, twoLetterWord)\n",
    "        #listOfTokens = removeWords(listOfTokens, countries_list)\n",
    "        #listOfTokens = removeWords(listOfTokens, nationalities_list)\n",
    "        #listOfTokens = removeWords(listOfTokens, other_words)\n",
    "        \n",
    "        listOfTokens = applyStemming(listOfTokens, param_stemmer)\n",
    "        #listOfTokens = removeWords(listOfTokens, other_words)\n",
    "\n",
    "        corpus[index]   = \" \".join(listOfTokens)\n",
    "        corpus[index] = unidecode(corpus[index])\n",
    "\n",
    "    return corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0080e09a-4cfe-4671-bd91-a10c70753253",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Install NLTK for Mac"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0e1ef38d-4dfb-4ee9-abee-34df1b0a9037",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import zipfile\n",
    "import requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "16b31817-454f-41f4-bf96-e59ee9189726",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "13905355"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Step 1: Download the punkt.zip file\n",
    "url = 'https://github.com/nltk/nltk_data/blob/gh-pages/packages/tokenizers/punkt.zip?raw=true'\n",
    "r = requests.get(url, allow_redirects=True)\n",
    "open('punkt.zip', 'wb').write(r.content)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7fc438f5-6982-4888-b6c1-1a284084c215",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: Extract the contents of the zip file\n",
    "with zipfile.ZipFile('punkt.zip', 'r') as zip_ref:\n",
    "    zip_ref.extractall('/Users/mac/nltk_data/tokenizers')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ce0e1197-a98a-42a0-a321-0c57d0d888b7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FileSystemPathPointer('/Users/mac/nltk_data/tokenizers/punkt/PY3')"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Step 3: Verify if the data is available\n",
    "from nltk.data import find\n",
    "find('tokenizers/punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "68c9d0ce-6abe-495e-bfd6-f49c726f4eca",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Error loading punkt: <urlopen error [SSL:\n",
      "[nltk_data]     CERTIFICATE_VERIFY_FAILED] certificate verify failed:\n",
      "[nltk_data]     unable to get local issuer certificate (_ssl.c:1000)>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5863f318-8120-4f67-88ea-d621fd766f76",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "628d290b-905c-417e-b8db-c68f851e13e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure the NLTK data path includes the directory where 'punkt' is located\n",
    "nltk.data.path.append('/Users/mac/nltk_data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "99311fd1-5f7e-4067-9936-fbf669790e82",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample text\n",
    "text = \"Hello, how are you?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "3f38cabe-b904-47ba-b666-2a0a2c76ca2e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hello', ',', 'how', 'are', 'you', '?']\n"
     ]
    }
   ],
   "source": [
    "# Tokenize the text\n",
    "tokens = word_tokenize(text)\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccb882ff-973d-44d0-81f9-f5b00c06c73e",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## 2. Lanjutan Corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "8aa1b332-8044-4526-b5ea-f3c3f31512c9",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "The language 'indonesia' is not supported.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[26], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m language \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mindonesia\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m----> 2\u001b[0m corpus \u001b[38;5;241m=\u001b[39m \u001b[43mprocessCorpus\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcorpus\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlanguage\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      3\u001b[0m corpus[\u001b[38;5;241m18\u001b[39m][\u001b[38;5;241m0\u001b[39m:\u001b[38;5;241m460\u001b[39m]\n",
      "Cell \u001b[0;32mIn[24], line 3\u001b[0m, in \u001b[0;36mprocessCorpus\u001b[0;34m(corpus, language)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mprocessCorpus\u001b[39m(corpus, language):   \n\u001b[1;32m      2\u001b[0m     \u001b[38;5;66;03m#stopwords = nltk.corpus.stopwords.words(language)\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m     param_stemmer \u001b[38;5;241m=\u001b[39m \u001b[43mSnowballStemmer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlanguage\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      4\u001b[0m     \u001b[38;5;66;03m#countries_list = [line.rstrip('\\n') for line in open('lists/countries.txt')] # Load .txt file line by line\u001b[39;00m\n\u001b[1;32m      5\u001b[0m     \u001b[38;5;66;03m#nationalities_list = [line.rstrip('\\n') for line in open('lists/nationalities.txt')] # Load .txt file line by line\u001b[39;00m\n\u001b[1;32m      6\u001b[0m     \u001b[38;5;66;03m#other_words = [line.rstrip('\\n') for line in open('lists/stopwords_scrapmaker.txt')] # Load .txt file line by line\u001b[39;00m\n\u001b[1;32m      8\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m document \u001b[38;5;129;01min\u001b[39;00m corpus:\n",
      "File \u001b[0;32m~/Downloads/jupyter_env/lib/python3.12/site-packages/nltk/stem/snowball.py:106\u001b[0m, in \u001b[0;36mSnowballStemmer.__init__\u001b[0;34m(self, language, ignore_stopwords)\u001b[0m\n\u001b[1;32m    104\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, language, ignore_stopwords\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[1;32m    105\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m language \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlanguages:\n\u001b[0;32m--> 106\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe language \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlanguage\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m is not supported.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    107\u001b[0m     stemmerclass \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mglobals\u001b[39m()[language\u001b[38;5;241m.\u001b[39mcapitalize() \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mStemmer\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m    108\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstemmer \u001b[38;5;241m=\u001b[39m stemmerclass(ignore_stopwords)\n",
      "\u001b[0;31mValueError\u001b[0m: The language 'indonesia' is not supported."
     ]
    }
   ],
   "source": [
    "language = 'indonesia'\n",
    "corpus = processCorpus(corpus, language)\n",
    "corpus[18][0:460]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e29912f5-4f57-4f2c-83d5-a9438871088c",
   "metadata": {},
   "source": [
    "the language 'indonesia' or 'indonesian' is not supported by NLTK's punkt tokenizer. NLTK's punkt tokenizer primarily supports a limited number of languages.\n",
    "\n",
    "For Indonesian language support, you can use other libraries like Sastrawi or spacy for tokenization."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be75f825-e151-42a3-ac04-205d9cf92893",
   "metadata": {},
   "source": [
    "# Use Spacy Library"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de8e733f-3020-42eb-b376-2f3205e14f46",
   "metadata": {},
   "source": [
    "### Pilih kolom yang akan digunakan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "84ef6089-2632-4fa2-9259-d00fb8bf74fe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       [Closing Engine] Kota Denpasar 2024 - Jalur Zo...\n",
       "1       [Kendala] Siswa Masih Bisa Langsung Pengajuan ...\n",
       "2       [Closing Engine] Prov NTT 2024 - SMA All Jalur...\n",
       "3       [DEV] Pengecekan dan Penyesuaian hasil seleksi...\n",
       "4       Pengecekan File Ajuan Pendaftaran di Operator ...\n",
       "                              ...                        \n",
       "2031    Reset daftar Ajuan Pendaftaran Gabungan zona Demo\n",
       "2032                    Override Engine Seleksi Kota Batu\n",
       "2033       Dokumen SPH PPDB Online Kota Palangkaraya 2023\n",
       "2034    Override Info Dari Formatter Kapasitas Jika Ya...\n",
       "2035                 Konfirmasi daerah 3 Kota/Kab di TR I\n",
       "Name: subject, Length: 2036, dtype: object"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = data['subject']\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "3f54285e-d120-41b7-9953-c57be592ec83",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['[Closing Engine] Kota Denpasar 2024 - Jalur Zonasi Kategori Umum - Tahap 1',\n",
       "       '[Kendala] Siswa Masih Bisa Langsung Pengajuan Pendaftaran di 2 Jenjang Sekaligus',\n",
       "       '[Closing Engine] Prov NTT 2024 - SMA All Jalur - Tahap 1', ...,\n",
       "       'Dokumen SPH PPDB Online Kota Palangkaraya 2023',\n",
       "       'Override Info Dari Formatter Kapasitas Jika Yang Diakses Adalah Kota Batu',\n",
       "       'Konfirmasi daerah 3 Kota/Kab di TR I'], dtype=object)"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.values #Menjadikan array"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcfeb2a0-959f-4d5e-ab59-0bbb61f359b6",
   "metadata": {},
   "source": [
    "### Cleaning data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "c46c05b3-85fe-4aae-829c-2511855608ef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       [closing engine] kota denpasar 2024 - jalur zo...\n",
       "1       [kendala] siswa masih bisa langsung pengajuan ...\n",
       "2       [closing engine] prov ntt 2024 - sma all jalur...\n",
       "3       [dev] pengecekan dan penyesuaian hasil seleksi...\n",
       "4       pengecekan file ajuan pendaftaran di operator ...\n",
       "                              ...                        \n",
       "2031    reset daftar ajuan pendaftaran gabungan zona demo\n",
       "2032                    override engine seleksi kota batu\n",
       "2033       dokumen sph ppdb online kota palangkaraya 2023\n",
       "2034    override info dari formatter kapasitas jika ya...\n",
       "2035                 konfirmasi daerah 3 kota/kab di tr i\n",
       "Name: subject, Length: 2036, dtype: object"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Mengubah huruf kapital menjadi lower case\n",
    "data = data.str.lower()\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "adb10fff-406b-4f89-9b54-80e22bb03bc9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       closing engine kota denpasar 2024  jalur zonas...\n",
       "1       kendala siswa masih bisa langsung pengajuan pe...\n",
       "2       closing engine prov ntt 2024  sma all jalur  t...\n",
       "3       dev pengecekan dan penyesuaian hasil seleksi j...\n",
       "4       pengecekan file ajuan pendaftaran di operator ...\n",
       "                              ...                        \n",
       "2031    reset daftar ajuan pendaftaran gabungan zona demo\n",
       "2032                    override engine seleksi kota batu\n",
       "2033       dokumen sph ppdb online kota palangkaraya 2023\n",
       "2034    override info dari formatter kapasitas jika ya...\n",
       "2035                  konfirmasi daerah 3 kotakab di tr i\n",
       "Name: subject, Length: 2036, dtype: object"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def clean_text(text):\n",
    "    return re.sub(r\"(@\\[A-Za-z0-9]+)|([^0-9A-Za-z \\t])|(\\w+:\\/\\/\\S+)|^rt|http.+?\", \"\", text)\n",
    "\n",
    "# Apply the function to each element in the 'text' column\n",
    "data = data.apply(clean_text)\n",
    "\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "69c483c2-8a4e-4b6a-9a61-e186d59c638c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['closing engine kota denpasar 2024  jalur zonasi kategori umum  tahap 1',\n",
       "       'kendala siswa masih bisa langsung pengajuan pendaftaran di 2 jenjang sekaligus',\n",
       "       'closing engine prov ntt 2024  sma all jalur  tahap 1', ...,\n",
       "       'dokumen sph ppdb online kota palangkaraya 2023',\n",
       "       'override info dari formatter kapasitas jika yang diakses adalah kota batu',\n",
       "       'konfirmasi daerah 3 kotakab di tr i'], dtype=object)"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "603bfbff-a653-4dda-b192-43bd239778aa",
   "metadata": {},
   "source": [
    "## Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "5fe0c8d3-bfe3-4bde-814d-47e111fd0c67",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Article process: 2036 from 2036\n",
      "\n",
      "total vacab stem:  17444\n",
      "total vacab tokenize:  17444\n"
     ]
    }
   ],
   "source": [
    "#not super pythonic, no, not at all.\n",
    "#use extend so it's a big flat list of vocab\n",
    "from __future__ import print_function\n",
    "totalvocab_stemmed = []\n",
    "totalvocab_tokenized = []\n",
    "dataarticle = list(data.values)\n",
    "count = 0\n",
    "for i in dataarticle:\n",
    "    print(\"\\rArticle process: {} from {}\".format(count+1, len(dataarticle)), end=\"\")\n",
    "    count += 1\n",
    "    allwords_stemmed = tokenize_and_stem(i) #for each item in 'synopses', tokenize/stem\n",
    "    totalvocab_stemmed.extend(allwords_stemmed) #extend the 'totalvocab_stemmed' list\n",
    "    \n",
    "    allwords_tokenized = tokenize_only(i)\n",
    "    totalvocab_tokenized.extend(allwords_tokenized)\n",
    "print (\"\")\n",
    "print (\"\")\n",
    "\n",
    "print (\"total vacab stem: \", len(totalvocab_stemmed))\n",
    "print (\"total vacab tokenize: \",len(totalvocab_tokenized))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "07b133d3-1889-4ce6-b407-1b08edd703ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "there are 17444 items in vocab_frame\n"
     ]
    }
   ],
   "source": [
    "vocab_frame = pd.DataFrame({'words': totalvocab_tokenized}, index = totalvocab_stemmed)\n",
    "print ('there are ' + str(vocab_frame.shape[0]) + ' items in vocab_frame')\n",
    "# vocab_frame[:10] #print 10 word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "26c3ca09-65ea-4aeb-a34a-23034e735deb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TF-idf matrix:  (2036, 1285)\n"
     ]
    }
   ],
   "source": [
    "from spacy.lang.id import stop_words\n",
    "from string import punctuation, digits\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "STWRD = list(set(punctuation))\n",
    "STWRD += list(set(digits))\n",
    "STWRD += stop_words.STOP_WORDS\n",
    "\n",
    "#tfidf_vectorizer = TfidfVectorizer(max_df=0.8, max_features=200000,\n",
    "                                 #min_df=2, stop_words=STWRD,\n",
    "                                 #use_idf=True, tokenizer=tokenize_and_stem, ngram_range=(1,3))\n",
    "\n",
    "tfidf_vectorizer = TfidfVectorizer()\n",
    "\n",
    "tfidf_matrix = tfidf_vectorizer.fit_transform(data.values) #fit the vectorizer to synopses\n",
    "\n",
    "print(\"TF-idf matrix: \",tfidf_matrix.shape)\n",
    "\n",
    "terms = tfidf_vectorizer.get_feature_names_out()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "b0cdc326-f678-48a2-baab-3c10fee5e513",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-2.22044605e-16,  1.00000000e+00,  6.15818297e-01, ...,\n",
       "         9.51257108e-01,  9.72805297e-01,  1.00000000e+00],\n",
       "       [ 1.00000000e+00,  0.00000000e+00,  1.00000000e+00, ...,\n",
       "         1.00000000e+00,  1.00000000e+00,  9.45878901e-01],\n",
       "       [ 6.15818297e-01,  1.00000000e+00, -2.22044605e-16, ...,\n",
       "         1.00000000e+00,  1.00000000e+00,  1.00000000e+00],\n",
       "       ...,\n",
       "       [ 9.51257108e-01,  1.00000000e+00,  1.00000000e+00, ...,\n",
       "        -2.22044605e-16,  9.59068721e-01,  1.00000000e+00],\n",
       "       [ 9.72805297e-01,  1.00000000e+00,  1.00000000e+00, ...,\n",
       "         9.59068721e-01, -2.22044605e-16,  1.00000000e+00],\n",
       "       [ 1.00000000e+00,  9.45878901e-01,  1.00000000e+00, ...,\n",
       "         1.00000000e+00,  1.00000000e+00, -2.22044605e-16]])"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "dist = 1 - cosine_similarity(tfidf_matrix)\n",
    "print\n",
    "dist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "8417d6d4-4175-422a-a354-686ae0021bf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "\n",
    "num_clusters = 4\n",
    "\n",
    "km = KMeans(n_clusters=num_clusters)\n",
    "\n",
    "km.fit(tfidf_matrix)\n",
    "\n",
    "clusters = km.labels_.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "6c9fe952-cad6-4781-b6db-a72b0d3316a5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>subject</th>\n",
       "      <th>cluster</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>closing engine kota denpasar 2024  jalur zonas...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>kendala siswa masih bisa langsung pengajuan pe...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>closing engine prov ntt 2024  sma all jalur  t...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>dev pengecekan dan penyesuaian hasil seleksi j...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>pengecekan file ajuan pendaftaran di operator ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>reset daftar ajuan pendaftaran gabungan zona demo</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>override engine seleksi kota batu</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>dokumen sph ppdb online kota palangkaraya 2023</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>override info dari formatter kapasitas jika ya...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>konfirmasi daerah 3 kotakab di tr i</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2036 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              subject  cluster\n",
       "1   closing engine kota denpasar 2024  jalur zonas...        1\n",
       "0   kendala siswa masih bisa langsung pengajuan pe...        0\n",
       "1   closing engine prov ntt 2024  sma all jalur  t...        1\n",
       "1   dev pengecekan dan penyesuaian hasil seleksi j...        1\n",
       "0   pengecekan file ajuan pendaftaran di operator ...        0\n",
       "..                                                ...      ...\n",
       "1   reset daftar ajuan pendaftaran gabungan zona demo        1\n",
       "1                   override engine seleksi kota batu        1\n",
       "2      dokumen sph ppdb online kota palangkaraya 2023        2\n",
       "1   override info dari formatter kapasitas jika ya...        1\n",
       "1                 konfirmasi daerah 3 kotakab di tr i        1\n",
       "\n",
       "[2036 rows x 2 columns]"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "films = { 'subject': list(data.values), 'cluster': clusters }\n",
    "\n",
    "frame = pd.DataFrame(films, index = [clusters] , columns = ['subject','cluster'])\n",
    "frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "95c94f15-d9b8-49ca-a121-eb1e199c66cf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "cluster\n",
       "1    1375\n",
       "3     298\n",
       "0     182\n",
       "2     181\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "frame['cluster'].value_counts() #number of article per cluster (clusters from 0 to 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "36fcf007-64d8-42f6-b798-346d9ab46444",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top terms per cluster:\n",
      "\n",
      "Cluster 0 words:"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "\"None of [Index(['tidak'], dtype='object')] are in the [index]\"",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[107], line 12\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCluster \u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m words:\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m i, end\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m ind \u001b[38;5;129;01min\u001b[39;00m order_centroids[i, :\u001b[38;5;241m6\u001b[39m]: \u001b[38;5;66;03m#replace 6 with n words per cluster\u001b[39;00m\n\u001b[0;32m---> 12\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m%\u001b[39m \u001b[43mvocab_frame\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mloc\u001b[49m\u001b[43m[\u001b[49m\u001b[43mterms\u001b[49m\u001b[43m[\u001b[49m\u001b[43mind\u001b[49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msplit\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m \u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\u001b[38;5;241m.\u001b[39mvalues\u001b[38;5;241m.\u001b[39mtolist()[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mencode(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mignore\u001b[39m\u001b[38;5;124m'\u001b[39m), end\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m,\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28mprint\u001b[39m() \u001b[38;5;66;03m#add whitespace\u001b[39;00m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;28mprint\u001b[39m() \u001b[38;5;66;03m#add whitespace\u001b[39;00m\n",
      "File \u001b[0;32m~/Downloads/jupyter_env/lib/python3.12/site-packages/pandas/core/indexing.py:1191\u001b[0m, in \u001b[0;36m_LocationIndexer.__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   1189\u001b[0m maybe_callable \u001b[38;5;241m=\u001b[39m com\u001b[38;5;241m.\u001b[39mapply_if_callable(key, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobj)\n\u001b[1;32m   1190\u001b[0m maybe_callable \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_deprecated_callable_usage(key, maybe_callable)\n\u001b[0;32m-> 1191\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_getitem_axis\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmaybe_callable\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maxis\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Downloads/jupyter_env/lib/python3.12/site-packages/pandas/core/indexing.py:1420\u001b[0m, in \u001b[0;36m_LocIndexer._getitem_axis\u001b[0;34m(self, key, axis)\u001b[0m\n\u001b[1;32m   1417\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(key, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mndim\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m key\u001b[38;5;241m.\u001b[39mndim \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m   1418\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot index with multidimensional key\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m-> 1420\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_getitem_iterable\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maxis\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1422\u001b[0m \u001b[38;5;66;03m# nested tuple slicing\u001b[39;00m\n\u001b[1;32m   1423\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_nested_tuple(key, labels):\n",
      "File \u001b[0;32m~/Downloads/jupyter_env/lib/python3.12/site-packages/pandas/core/indexing.py:1360\u001b[0m, in \u001b[0;36m_LocIndexer._getitem_iterable\u001b[0;34m(self, key, axis)\u001b[0m\n\u001b[1;32m   1357\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_validate_key(key, axis)\n\u001b[1;32m   1359\u001b[0m \u001b[38;5;66;03m# A collection of keys\u001b[39;00m\n\u001b[0;32m-> 1360\u001b[0m keyarr, indexer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_listlike_indexer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1361\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobj\u001b[38;5;241m.\u001b[39m_reindex_with_indexers(\n\u001b[1;32m   1362\u001b[0m     {axis: [keyarr, indexer]}, copy\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, allow_dups\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m   1363\u001b[0m )\n",
      "File \u001b[0;32m~/Downloads/jupyter_env/lib/python3.12/site-packages/pandas/core/indexing.py:1558\u001b[0m, in \u001b[0;36m_LocIndexer._get_listlike_indexer\u001b[0;34m(self, key, axis)\u001b[0m\n\u001b[1;32m   1555\u001b[0m ax \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobj\u001b[38;5;241m.\u001b[39m_get_axis(axis)\n\u001b[1;32m   1556\u001b[0m axis_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobj\u001b[38;5;241m.\u001b[39m_get_axis_name(axis)\n\u001b[0;32m-> 1558\u001b[0m keyarr, indexer \u001b[38;5;241m=\u001b[39m \u001b[43max\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_indexer_strict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis_name\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1560\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m keyarr, indexer\n",
      "File \u001b[0;32m~/Downloads/jupyter_env/lib/python3.12/site-packages/pandas/core/indexes/base.py:6200\u001b[0m, in \u001b[0;36mIndex._get_indexer_strict\u001b[0;34m(self, key, axis_name)\u001b[0m\n\u001b[1;32m   6197\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   6198\u001b[0m     keyarr, indexer, new_indexer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reindex_non_unique(keyarr)\n\u001b[0;32m-> 6200\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_raise_if_missing\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkeyarr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindexer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis_name\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   6202\u001b[0m keyarr \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtake(indexer)\n\u001b[1;32m   6203\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(key, Index):\n\u001b[1;32m   6204\u001b[0m     \u001b[38;5;66;03m# GH 42790 - Preserve name from an Index\u001b[39;00m\n",
      "File \u001b[0;32m~/Downloads/jupyter_env/lib/python3.12/site-packages/pandas/core/indexes/base.py:6249\u001b[0m, in \u001b[0;36mIndex._raise_if_missing\u001b[0;34m(self, key, indexer, axis_name)\u001b[0m\n\u001b[1;32m   6247\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m nmissing:\n\u001b[1;32m   6248\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m nmissing \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mlen\u001b[39m(indexer):\n\u001b[0;32m-> 6249\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNone of [\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mkey\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m] are in the [\u001b[39m\u001b[38;5;132;01m{\u001b[39;00maxis_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m]\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   6251\u001b[0m     not_found \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(ensure_index(key)[missing_mask\u001b[38;5;241m.\u001b[39mnonzero()[\u001b[38;5;241m0\u001b[39m]]\u001b[38;5;241m.\u001b[39munique())\n\u001b[1;32m   6252\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnot_found\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m not in index\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mKeyError\u001b[0m: \"None of [Index(['tidak'], dtype='object')] are in the [index]\""
     ]
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "\n",
    "print(\"Top terms per cluster:\")\n",
    "print()\n",
    "#sort cluster centers by proximity to centroid\n",
    "order_centroids = km.cluster_centers_.argsort()[:, ::-1] \n",
    "\n",
    "for i in range(num_clusters):\n",
    "    print(\"Cluster %d words:\" % i, end='')\n",
    "    \n",
    "    for ind in order_centroids[i, :6]: #replace 6 with n words per cluster\n",
    "        print(' %s' % vocab_frame.loc[terms[ind].split(' ')].values.tolist()[0][0].encode('utf-8', 'ignore'), end=',')\n",
    "    print() #add whitespace\n",
    "    print() #add whitespace\n",
    "    \n",
    "    print(\"Cluster %d titles:\" % i, end='')\n",
    "    for title in frame.loc[i]['subject'].values.tolist():\n",
    "        print(' %s,' % title, end='')\n",
    "    print() #add whitespace\n",
    "    print() #add whitespace\n",
    "    \n",
    "print()\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "638fd7dc-8587-45ec-90b9-99cbff8f08a7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
